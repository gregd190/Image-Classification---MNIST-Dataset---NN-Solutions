{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Recognition - MNIST Dataset - Simple Neural Network - No Preprocessing\n",
    "\n",
    "This will be a quick demonstration of building a conventional fully-connected neural network for character recognition using Keras.\n",
    "\n",
    "The MNIST dataset provides us with a large set of labelled images that are at a low-resolution, which keeps the training time reasonable, even on a modest CPU. More information and raw data available at http://yann.lecun.com/exdb/mnist/, although the files needed are in the repository. \n",
    "\n",
    "The dataset is broken into 4 files, sets of images and labels for both the training and test sets. \n",
    "\n",
    "Firstly we import the various libraries we will need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import struct\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset files are not in an immediately useable format. We will use the following function to read the files into arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST data is provided in a file format that needs to be read before the data can be fed to the neural network. \n",
    "def read_data(image_filename, label_filename):\n",
    "    \n",
    "    #Create arrays to store labels and pixel data\n",
    "    label_array = []\n",
    "    image_array = []\n",
    "    \n",
    "    # Load everything in some numpy arrays\n",
    "    with open(label_filename, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(image_filename, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    for i in range(len(lbl)):\n",
    "        label_array.append(lbl[i])\n",
    "        image_array.append(np.ndarray.flatten(img[i]))\n",
    "    \n",
    "    #Convert arrays to np.array\n",
    "    image_array = np.array(image_array)\n",
    "    \n",
    "    return(image_array, label_array)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_labels =  (60000,)\n",
      "Shape of train_images =  (60000, 784)\n",
      "train_labels[0:5] =  [5, 0, 4, 1, 9]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = read_data('train-images.idx3-ubyte', 'train-labels.idx1-ubyte')\n",
    "print('Shape of train_labels = ',np.shape(train_labels))\n",
    "print('Shape of train_images = ', np.shape(train_images))\n",
    "\n",
    "\n",
    "print('train_labels[0:5] = ', train_labels[0:5])\n",
    "\n",
    "print(train_images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note each is an integer between 0 and 9. As the network will output probabilities between 0 and 1 for each character, \n",
    "let's change the labels to be 'one-hot' encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_one_hot(labels):\n",
    "\n",
    "    labels_one_hot = np.zeros((len(labels),10))\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels_one_hot[i,labels[i]] = 1\n",
    "    \n",
    "    return(labels_one_hot)\n",
    "\n",
    "train_labels = make_one_hot(train_labels)\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 5 labels now. Based on the prior printout, we should have 1's in the 6th, 1st, 5th, 2nd and last positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we can proceed to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 13,002\n",
      "Trainable params: 13,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape, num_output_nodes, lr, size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(size[0], input_shape = input_shape, activation = 'relu'))\n",
    "    \n",
    "    for i in range(1,len(size)):\n",
    "        model.add(Dense(size[i], activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(num_output_nodes, activation = 'softmax')) \n",
    "    \n",
    "    adam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    model.compile(loss = 'mse', optimizer = adam, metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model((784,), 10, 0.001, [16,16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. A validation split of 0.1 is used. This means that 10% (6000 images) is not used for training but \n",
    "instead is used to check the performance after each epoch. This will allow us to detect overfitting to the training set. We'll plot the accuracy of both the training and validation sets (you could plot the loss instead). If you wish to watch the change in loss and accuracy during training, set verbose to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'], 'r', label='training accuracy')\n",
    "plt.plot(history.history['val_acc'], 'b', label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(history.history['val_acc'][399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 400 epochs, we have reached a validation accuracy of around 93% (although note that this could very between instances due to random selection of the validation set and the random initialization of the network). It is worth noting that the loss and accuracy for the validation set are similar to those of the training set, so we are not overfitting excessively, although it would appear that any further training is of little benefit with the parameters used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the effect of learning rate. The initial learning rate of 0.001 was a good starting point, but we might be able to get either a similar result in less epochs with a larger learning rate, or a better end result with a smaller one. Batch size is kept constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_001 = model\n",
    "model_0_01 = build_model((784,), 10, 0.01, [16,16])\n",
    "model_0_0001 = build_model((784,), 10, 0.0001, [16,16])\n",
    "\n",
    "history_0_001 = history\n",
    "history_0_01 = model_0_01.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "history_0_0001 = model_0_0001.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "\n",
    "plt.plot(history.history['acc'], 'r', label='LR=0.001 - training accuracy')\n",
    "plt.plot(history.history['val_acc'], 'b', label='LR=0.001 - validation accuracy')\n",
    "plt.plot(history_0_01.history['acc'], 'g', label='LR=0.01 training accuracy')\n",
    "plt.plot(history_0_01.history['val_acc'], 'c', label='LR=0.01 - validation accuracy')\n",
    "plt.plot(history_0_0001.history['acc'], 'y', label='LR=0.0001 - training accuracy')\n",
    "plt.plot(history_0_0001.history['val_acc'], 'm', label='LR=0.0001 - validation accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears our initial learning rate was probably about right. The smaller learning rate might have hit a slightly better final result eventually, but even after 1000 epochs it had not reached the level of the 0.001 learning rate\n",
    "\n",
    "Now let's investigate the effect of network size. The first network we used had 2 hidden layers, each containing 16 layers. We'll run both a smaller (single layer, 16 nodes) and a larger network (3 hidden layers, 16 nodes each) and compare results. We will use the best learning rate from the prior example (0.001)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_layer = build_model((784,), 10, 0.001, [16])\n",
    "model_3_layers = build_model((784,), 10, 0.001, [16,16,16])\n",
    "model_2_32_layers = build_model((784,), 10, 0.001, [32,32])\n",
    "\n",
    "history_2_layers = history\n",
    "history_1_layer = model_1_layer.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "history_3_layers = model_3_layers.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "history_2_32_layers = model_2_32_layers.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "\n",
    "plt.plot(history_1_layer.history['acc'], 'r', label='1 layer - training accuracy')\n",
    "plt.plot(history_1_layer.history['val_acc'], 'b', label='1 layer - validation accuracy')\n",
    "plt.plot(history_2_layers.history['acc'], 'g', label='2 layers - training accuracy')\n",
    "plt.plot(history_2_layers.history['val_acc'], 'c', label='2 layers - validation accuracy')\n",
    "plt.plot(history_3_layers.history['acc'], 'y', label='3 layers - training accuracy')\n",
    "plt.plot(history_3_layers.history['val_acc'], 'm', label='3 layers - validation accuracy')\n",
    "plt.plot(history_2_32_layers.history['acc'], 'y', label='2 32 layers - training accuracy')\n",
    "plt.plot(history_2_32_layers.history['val_acc'], 'm', label='2 32 layers - validation accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_1_layer.history['val_acc'][399],\n",
    "history_2_layers.history['val_acc'][399],\n",
    "history_3_layers.history['val_acc'][399],\n",
    "history_2_32_layers.history['val_acc'][399])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is little final performance benefit to the additional layers, although larger layers does significantly improve the performance. Let's see if the effect extends even further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_128_layers = build_model((784,), 10, 0.001, [128,128])\n",
    "\n",
    "history_2_128_layers = model_2_128_layers.fit(train_images, train_labels, batch_size=5120, validation_split=0.1, epochs=400, verbose=0)\n",
    "\n",
    "plt.plot(history_2_32_layers.history['acc'], 'y', label='2 32 layers - training accuracy')\n",
    "plt.plot(history_2_32_layers.history['val_acc'], 'm', label='2 32 layers - validation accuracy')\n",
    "plt.plot(history_2_128_layers.history['acc'], 'r', label='2 128 layers - training accuracy')\n",
    "plt.plot(history_2_128_layers.history['val_acc'], 'b', label='2 128 layers - validation accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_2_32_layers.history['val_acc'][399])\n",
    "print(history_2_128_layers.history['val_acc'][399])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the larger network at best provides no benefit, and may in fact be detrimental. \n",
    "\n",
    "Using our best network (2 hidden layers of 32 nodes), let's have a look at the images that the model is getting wrong.  Let's look at the first 10 images the model is incorrectly classifying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwrong = 0\n",
    "a = 0\n",
    "wrong_indexes = [] #List of indexes of wrong predictions from training set\n",
    "predictions = model_2_32_layers.predict(train_images)\n",
    "\n",
    "while numwrong<10:\n",
    "    if np.argmax(predictions[a]) != np.argmax(train_labels[a]):\n",
    "        wrong_indexes.append(a)\n",
    "        numwrong += 1\n",
    "    a += 1\n",
    "\n",
    "for i in wrong_indexes:\n",
    "    plt.imshow(np.reshape(train_images[i], (28,28)), cmap='gray')\n",
    "    plt.show()\n",
    "    print('Label = ', np.argmax(train_labels[i]), 'Prediction = ', np.argmax(predictions[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the misclassified images are poorly drawn, and in many cases would not be classified correctly by a human. It is therefore unlikely that the performance could be improved substantially from this point. \n",
    "\n",
    "### Performance on the test set\n",
    "\n",
    "Finally, let's see how our model performs on the test set. It should be close to the validation set performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = read_data('t10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte')\n",
    "\n",
    "test_labels = make_one_hot(test_labels)\n",
    "\n",
    "predictions = model_2_32_layers.evaluate(test_images, test_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_2_32_layers.metrics_names)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the test set is about 96.3%. \n",
    "\n",
    "Again, let's have a look at the ones the model is incorrectly classifying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "numwrong = 0\n",
    "a = 0\n",
    "wrong_indexes = [] #List of indexes of wrong predictions from training set\n",
    "predictions = model_2_32_layers.predict(test_images)\n",
    "\n",
    "while numwrong < 10:\n",
    "    if np.argmax(predictions[a]) != np.argmax(test_labels[a]):\n",
    "        wrong_indexes.append(a)\n",
    "        numwrong += 1\n",
    "    a += 1\n",
    "\n",
    "for i in wrong_indexes:\n",
    "    plt.imshow(np.reshape(test_images[i], (28,28)), cmap='gray')\n",
    "    plt.show()\n",
    "    print('Label = ', np.argmax(test_labels[i]), 'Prediction = ', np.argmax(predictions[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noted that while some of the mischaracterised images are poorly drawn, some are reasonably drawn but significantly angled. Preprocessing the training data to rotate some of the training images slightly may improve performance. \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "A conventional neural network was trained to recognise digit characters in the MNIST dataset. It was found that a larger neural network improved performance, but only to a point. It was also noted that a significant proportion of the characters the network mislabelled were misdrawn, and would not be consistently classified correctly by humans. The test-set accuracy was 96.3%. Further improvement could be achieved by preprocessing the training data, in particular to include a small range of rotation. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
